---
title: 'Responsible AI Toolkits for Public Service'
year: '2025'
tags: ['AI', 'Governance']
summary: 'Designing toolkits that help public agencies audit models, document decisions, and give residents appeal paths.'
image: 'url(/images/thumb-sticky.jpg)'
---

We partnered with public agencies to design toolkits that make AI systems accountable: checklists for model intake, audit trails for decisions, and clear resident-facing appeal flows.

## Problem

Agencies were deploying models for eligibility, prioritization, and triage without consistent documentation or channels for redress. Staff needed lightweight ways to capture intent, data lineage, and human oversight steps before deployment.

## Approach

- Rapid audits of existing decision flows to locate where AI touched resident outcomes.
- Co-design workshops with policy, legal, and frontline staff to define review gates.
- Prototype toolkits: intake forms, risk checklists, audit log templates, and appeal UX.

## What we shipped

- Model intake doc that records purpose, data sources, evaluation slices, and owners.
- Inline "why this decision" UI with citations and a one-click "Request human review" entry point that routes context.
- Exportable audit logs that capture prompts, retrieved sources, overrides, and time-stamped human interventions.

## Outcomes

- Teams adopted review gates before launch; 4 models paused to collect missing evaluations.
- Residents received transparent receipts with appeal IDs; escalations routed faster.
- Compliance teams gained a single export format for audits instead of scattered notes.

Responsible AI in government is as much about workflow as it is about models. Toolkits that meet staff where they work made accountability sustainable.
